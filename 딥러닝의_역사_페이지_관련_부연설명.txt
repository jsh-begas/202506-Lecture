- 마크다운 형식의 파일입니다. 
- 마크다운으로 조판해서 보고싶으면 주피터의 markdown cell에 넣고 실행하세요.
- 중요한 부분은 ** **로 묶어서 표시했고, 마크다운 조판시 굵은 글씨로 표시됩니다.


### Threshold logic (McCulloch & Pitt, 1943)
- 뉴런 하나를 최대한 단순한 형태로 흉내내는 모형
- 뉴런 : 여러 입력을 받아서 통합한 뒤, 역치를 넘으면 반응
- 모형 : 여러 입력 $x_i$을 받아서 선형결합 ($\sum w_i x_i$)한 뒤, 기준값 $\theta$을 넘으면 1, 안넘으면 0 출력
  - 1층 구조
  - 입출력의 개념은 있었으나, 가중치 $w_i$ 및 기준값 $\theta$의 학습은 체계적으로 고려되지 않음
- AND, OR, NOT 등의 논리 게이트(0,1 변수 입력을 한 개~두 개 받아 0,1 변수를 출력)를 대신할 수 있음을 밝힘
  - 복잡한 프로그램도 이 논리 게이트를 여러층으로 쌓아서 만들 수 있으니 이 로직으로도 많은 알고리즘을 구현 할 수 있지 않을까?


### Perceptron (Rosenblatt, 1958)
- threshold logic을 구현하는 컴퓨터 (Mark I. ENIAC 다음에 나오는 Harvard Mark I이랑은 다른 컴퓨터)
  - 빛을 받아들이는 400개(20x20)의 칸을 만들어서, 빛의 세기를 값으로 측정한 뒤 threshold logic대로 연산해서 0,1을 출력하도록 만듬
- 모형 : threshold logic과 구조는 동일
  - 가중치 $w_i$ 를 원하는 출력과 현재 출력을 대조해서 조정함으로써 '학습'하는 절차가 도입됨 (수동 학습 -> 지도 학습)
- 복잡한 프로그램을 원하는 입출력 쌍만 계속 집어넣어서 학습시켜주면 만들어줄 수 있는 기계??


### XOR예제 (Minsky & Papert, 1969)
- Perceptrons라는 책
  - perceptron이 가지는 성능을 수학적으로 증명
  - 한계점도 많이 나타남
- perceptron (1층 threshold logic)으로 학습할 수 없는 패턴이 많이 있음
  - XOR 패턴 표현 안됨
  - 연결성 확인 문제 : 빛이 켜진 영역이 연결되었는지 확인하는 로직
- 따라서, 여러 층 로직이 필요한데...
  - 아직 여러층 모형의 효율적인 학습방법을 모름
  - 이 저자들의 당시 저명도가 높았음
  - 다른 종류의 AI 연구가 탄력을 받음 (symbolic AI : 사람이 이해할 수 있는 형태로 문제를 표현하는 접근법. 큰 If-logic이나 사전, 분류법을 이용하는 방법)
  - (subsymbolic AI는 사람이 이해할 수는 없지만 데이터 기반으로 학습해서 큰 문제로의 일반화가 비교적 용이한 방법)
- 결과적으로 AI winter가 발생

### Hopfield Network (Hopfield, 1983)
- '패턴을 기억하는 network'
- -1, 1을 출력하는 뉴런들을 일정한 세기(가중치)로 연결
  - 뉴런 단위로 연결된 다른 뉴런 출력 x 가중치의 합을 기준으로 양수이면 1, 음수이면 -1을 출력
  - 익힐 패턴을 넣어보고 같은 부호이면 연결 세기를 강화시키는 방식으로 학습
- 한 방향으로만 입력, 출력이 이뤄지던 이전 구조와는 달리 출력하는 방향으로 따라가다보면 이전 노드로 돌아갈 수도 있는 RNN 구조
- subsymbolic AI로의 관심을 일부 이끌어냄

### Multi-Layer Perceptron / **Backpropagation** (Hinton, 1986)
- 다층 Perceptron에서의 학습 문제를 풀 수 있는 'Backpropagation of error'라는 알고리즘을 설명
- 일층이면 오차로 가중치를 업데이트 했는데, 다층에서는 중간층의 경우 대조할 학습용 출력값 없이 어떻게 업데이트할까?
  - 그 층의 출력값 대신 최종 output 단계로의 영향을 따져보자
  - 각 층에서 가중치 및 입력이 그 층의 출력에 미치는 영향을 계산하는 것은 쉬움 (일차식과 0/1 계단함수의 합성)
  - 각 층의 가중치의 변화가 최종 출력에 미치는 영향은, (현재층의 가중치의 현재층 출력으로의 영향) x (그 다음층의 입력이 최종 출력에 미치는 영향)과 같이 표현 가능 (합성함수의 미분)
  - 그런고로 각 층 가중치별로 한층 짜리 미분을 계산한 뒤, 최종 출력으로의 영향을 가장 출력에 가까운 순서로 역순으로 축적해나가면 최종 층의 데이터로의 영향을 평가 할 수 있고, 중간 층 출력에 대한 별도의 평가 없이 중간 층을 업데이트 할 수 있음
- '이제 다층도 체계적으로 학습할 수 있다' -> AI winter에서 벗어나게 함

### AutoEncoder (Baldi & Hornic, 1989)
- 입력과 출력에 같은 데이터를 사용하고, 중간에 병목 (노드가 적은 영역)을 배치하는 구조
- 적은 차원으로 데이터를 표현했음에도 원본이 잘 복원됨 -> 병목만 출력 시켜서 데이터를 압축 (차원 축소) 시키는데 사용
- 특징이 확실한 데이터만 입력시키면, 다른 특징의 데이터는 잘 복원 못시킴 -> 복원 가능 수준으로 이상치 판정에 사용

### LeNet (LeCun, 1989)
- 입력에 이미지 (픽셀 밝기 값)를 사용하고, 인접한 픽셀만 연결하는 중간층을 배치해서 점점 정보를 모아가는 구조 (CNN)
- 숫자 손글씨 진짜 숫자로 분별하는 문제에 적용

### Universal approximation (Cybenko, 1989)
- 입력 -> 출력 방향이 단방향인 구조 (feed-forward network)로 모든 연속 함수를 근사할 수 있다는 결과
- 단, 조건으로 층을 많이 쌓을 수 있어야 된다는 조건이 필요함

### **Vanishing gradient** (Bengio, 1994)
- 층수가 많은 구조나 RNN과 같이 출력하다보면 노드가 순환되는 구조에서는 Backpropagation으로 가중치의 영향을 쌓다보면 중간구조 전체의 영향력을 곱으로 합쳐야 하다보니 최종층에의 영향력이 사라지거나 지나치게 증폭되는 문제가 발견됨

### **Overfitting**
- 복잡한 모형을 구성하면 다른 데이터(test data)를 잘 맞추지 못하도록 학습되는 경우가 많았음

### LSTM (Hochreiter, 1997)
- RNN에서 많은 층을 거치면서 vanishing gradient 현상을 겪는다면, 먼 미래까지 gradient의 정보를 상대적으로 잘 연결하는 구조(memory gate)를 주는것은 어떨까?

### Restricted Boltzman Machine (Hinton, 1999)
- (0,1)로 이뤄진 여러 변수들의 데이터/분포를 이 변수 + 숨겨진 1개의 층의 상호 연결관계로 근사하는 2층 인공신경망
- 분포를 근사하고나면 데이터 생성에 사용 가능
- 생성형 인공신경망의 시초

### Deep Belief Machine (Hinton, 2006)
- RBM을 여러층으로 쌓은 생성형 인공신경망
- 중요한 점은 여러층인데도 학습이 성공적으로 이루어졌다는 점
  - pretraining 기법을 사용 : 1층짜리 모형 학습 -> 층을 동결시키고 다시 1층을 쌓아서 해당층만 모형 학습 > 반복 > 끝나면 동결을 풀고 전체 학습

### Deep Boltzman Machine (Hinton, 2009)
- RBM에 다층구조를 주고, 단방향 연결 제한을 해제한 생성형 인공신경망

### Rectified Linear Unit=**ReLU** (Hinton & Bengio, 2010)
- 0까지는 0으로 계속 상수이다가 양수부분에서는 y=x 처럼 상승하는 활성화함수
- Vanishing Gradient 문제를 극적으로 개선하면서 구조적인 개선이나 특별한 학습 방법 없이도 깊은 층의 NN을 쓸 수 있게 함

### **Dropout** (Hinton, 2012)
- 학습할 데이터를 통과 시킬 때마다 랜덤하게 연결을 끊어줌 (예를 들어 연결마다 50%확률). 실전 예측할 때는 전부 연결
- 각 노드가 독립적으로 정보를 학습할 수 있도록 강제하고, shrinkage처럼도 행동함. 결과적으로 과적합을 방지함

### R-CNN (Girlshick, 2014~)
- 이미지 데이터를 받아서 물체가 있을 법한 상자 영역과 해당 물체의 종류를 출력
- 후보 영역을 결정하고(NN일수도 아닐수도), 그 상자 영역에서 CNN으로 분류 수행

### GAN (Goodfellow, 2015~)
- (이미지) 데이터 생성 인공신경망과 진위 판정용 인공신경망을 대결시켜서 인공 데이터 생성 능력을 높이는 방법
- 꼭 대결하는 모형이 인공신경망일 필요는 없지만, 이미지 규모의 큰 데이터에 대해서 적응할 수 있는 좋은 구조가 현재는 인공신경망 뿐이라 실질적으로 인공신경망 기술에 해당. 추후에는 두 모형의 인공신경망 구조에 변화를 주면서 발전해갔음

### AlphaGo (2016)
- deep learning과 강화 학습을 이용한 AI agent의 구현

### eXplainable AI (2017)
- deep learning 모형이 계속 복잡해지면서 어떻게 출력을 내는지 설명하기 어려워지자, 이를 설명하려는 수단을 만들려는 움직임
